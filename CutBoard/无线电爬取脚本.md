# 无线电爬取脚本

```python
"""
爬取业余无线电中继列表:
http://weixin.cqcqcq.cn/index.php?m=radio&c=index&a=list4city&city_id=80
Beautiful Soup 4 使用参考: http://c.biancheng.net/python_spider/bs4.html
安装依赖：
pip install httpx bs4
"""
import httpx
from bs4 import BeautifulSoup
import csv
from pathlib import Path

# 广州
# /index.php?m=radio&c=index&a=list4city&city_id=76
# 佛山
# /index.php?m=radio&c=index&a=list4city&city_id=80
# 深圳
# /index.php?m=radio&c=index&a=list4city&city_id=77
# 珠海
# /index.php?m=radio&c=index&a=list4city&city_id=96
# 湛江
# /index.php?m=radio&c=index&a=list4city&city_id=93
# 肇庆
# /index.php?m=radio&c=index&a=list4city&city_id=94
# 江门
# /index.php?m=radio&c=index&a=list4city&city_id=83

urlsss = [
    ["广州", "/index.php?m=radio&c=index&a=list4city&city_id=76"],
    ["佛山", "/index.php?m=radio&c=index&a=list4city&city_id=80"],
    ["深圳", "/index.php?m=radio&c=index&a=list4city&city_id=77"],
    ["珠海", "/index.php?m=radio&c=index&a=list4city&city_id=96"],
    ["湛江", "/index.php?m=radio&c=index&a=list4city&city_id=93"],
    ["肇庆", "/index.php?m=radio&c=index&a=list4city&city_id=94"],
    ["江门", "/index.php?m=radio&c=index&a=list4city&city_id=83"],
    ["汕头", "/index.php?m=radio&c=index&a=list4city&city_id=88"],
    ["韶关", "/index.php?m=radio&c=index&a=list4city&city_id=90"],
    ["茂名", "/index.php?m=radio&c=index&a=list4city&city_id=85"],
    ["惠州", "/index.php?m=radio&c=index&a=list4city&city_id=82"],
    ["梅州", "/index.php?m=radio&c=index&a=list4city&city_id=86"],
    ["汕尾", "/index.php?m=radio&c=index&a=list4city&city_id=89"],
    ["河源", "/index.php?m=radio&c=index&a=list4city&city_id=81"],
    ["阳江", "/index.php?m=radio&c=index&a=list4city&city_id=91"],
    ["清远", "/index.php?m=radio&c=index&a=list4city&city_id=87"],
    ["东莞", "/index.php?m=radio&c=index&a=list4city&city_id=79"],
    ["中山", "/index.php?m=radio&c=index&a=list4city&city_id=95"],
    ["潮州", "/index.php?m=radio&c=index&a=list4city&city_id=78"],
    ["揭阳", "/index.php?m=radio&c=index&a=list4city&city_id=84"],
    ["云浮", "/index.php?m=radio&c=index&a=list4city&city_id=92"],
    ["顺德", "/index.php?m=radio&c=index&a=list4city&city_id=3411"]

]


param = "/index.php?m=radio&c=index&a=list4city&city_id=76"

url = "http://weixin.cqcqcq.cn%s"

header = {"UserAgent": "Mozilla/5.0 (Linux; Android 6.0.1;)"}

current_dir = Path(__file__).parent


def getListElement(list: list, num: int) -> str:
    """get list element by id without error"""
    try:
        return list[num].text.strip()
    except IndexError:
        return None


def parser(url) -> dict:
    """解析单个中继的页面,返回单个中继参数 dict"""
    with httpx.Client(headers=header) as client:
        resp = client.get(url)

    soup = BeautifulSoup(resp.text, 'html.parser')
    tags = soup.find(class_="card")
    name = tags.find(class_="comment").text.strip()
    info = tags.find_all(class_="item-after light")

    fq_rec = getListElement(info, 1)
    fq_diff = getListElement(info, 2)
    fq_transmit = getListElement(info, 3)
    mute = getListElement(info, 4)

    return {
        "name": name,
        "R": fq_rec,
        "diff": fq_diff,
        "T": fq_transmit,
        "CSDT": mute
    }


def getCityRelayUrl_Generator(cityUrl):
    """找到一个城市中继页面的URL"""
    with httpx.Client(headers=header) as client:
        resp = client.get(url % cityUrl)
    # create parser object
    soup = BeautifulSoup(resp.text, 'html.parser')
    tags = soup.find(
        class_="list-block media-list comment"
    ).find_all("a")

    for tag in tags:
        yield url % tag["href"]


def writeCSV(file, res_dict: dict = None, header=False, title: str = None):
    if title:
        file.write("\n%s\n" % title)

    fieldnames = ["名称", "接收频率", "频差", "发射频率", "哑音"]
    writer = csv.DictWriter(file, fieldnames=fieldnames)

    if header:
        writer.writeheader()

    if res_dict:
        writer.writerow({
            "名称": res_dict["name"],
            "接收频率": res_dict["R"],
            "频差": res_dict["diff"],
            "发射频率": res_dict["T"],
            "哑音": res_dict["CSDT"],
        }
        )


def main():
    # 写入 csv 文件
    # ANSI 使得 excel 打开不会乱码
    # newline 定义新一行的空格
    with open("%s" % Path(current_dir, "广东中继总表.csv"), 'w', encoding="ANSI", newline='') as csvfile:
        for url in urlsss:
            print("正在爬取...%s" % url[0])
            writeCSV(csvfile, header=True, title="%s" % url[0])
            for relay_url in getCityRelayUrl_Generator(url[1]):
                data_dict = parser(relay_url)
                print(data_dict)
                writeCSV(csvfile, data_dict)
            print("%s爬取结束!" % url[0])


if __name__ == "__main__":
    main()
    print("DONE!")
```

‍
